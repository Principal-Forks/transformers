{
  "vv": {
    "name": "Qwen2-VL Real Architecture (Code-Derived)",
    "version": "1.0.0",
    "nodeTypes": {
      "component": {
        "shapes": [
          "rounded"
        ]
      }
    },
    "edgeTypes": {
      "uses": {
        "style": "solid",
        "color": "#10b981",
        "width": 2,
        "directed": true
      },
      "processes": {
        "style": "solid",
        "color": "#06b6d4",
        "width": 2,
        "directed": true
      },
      "extends": {
        "style": "dashed",
        "color": "#ef4444",
        "width": 2,
        "directed": true
      },
      "configures": {
        "style": "dashed",
        "color": "#fbbf24",
        "width": 2,
        "directed": true
      },
      "encodes": {
        "style": "solid",
        "color": "#06b6d4",
        "width": 2,
        "directed": true
      },
      "generates": {
        "style": "solid",
        "color": "#06b6d4",
        "width": 2,
        "directed": true
      }
    }
  },
  "nodes": [
    {
      "id": "user_input",
      "type": "text",
      "x": 500,
      "y": 50,
      "width": 300,
      "height": 60,
      "color": "2",
      "text": "User Input (images/videos + text)",
      "vv": {
        "nodeType": "component",
        "shape": "rounded",
        "component": "API",
        "fill": "#ff9800",
        "stroke": "#ff9800"
      }
    },
    {
      "id": "processor",
      "type": "text",
      "x": 500,
      "y": 160,
      "width": 300,
      "height": 90,
      "color": "5",
      "text": "Qwen2VLProcessor\nprocessing_qwen2_vl.py:48\nInherits: ProcessorMixin",
      "vv": {
        "nodeType": "component",
        "shape": "rounded",
        "component": "Processor",
        "fill": "#06b6d4",
        "stroke": "#06b6d4"
      }
    },
    {
      "id": "image_processor",
      "type": "text",
      "x": 100,
      "y": 320,
      "width": 280,
      "height": 100,
      "color": "5",
      "text": "Qwen2VLImageProcessor\nimage_processing_qwen2_vl.py:105\nInherits: BaseImageProcessor\n\u2022 smart_resize()\n\u2022 resize, rescale, normalize",
      "vv": {
        "nodeType": "component",
        "shape": "rounded",
        "component": "ImageProcessor",
        "fill": "#06b6d4",
        "stroke": "#06b6d4"
      }
    },
    {
      "id": "tokenizer",
      "type": "text",
      "x": 920,
      "y": 320,
      "width": 250,
      "height": 100,
      "color": "5",
      "text": "Qwen2TokenizerFast\n(external)\n\u2022 Tokenizes text\n\u2022 Handles special tokens",
      "vv": {
        "nodeType": "component",
        "shape": "rounded",
        "component": "Tokenizer",
        "fill": "#06b6d4",
        "stroke": "#06b6d4"
      }
    },
    {
      "id": "video_processor",
      "type": "text",
      "x": 420,
      "y": 320,
      "width": 260,
      "height": 100,
      "color": "5",
      "text": "Qwen2VLVideoProcessor\nvideo_processing_qwen2_vl.py\n\u2022 Frame extraction\n\u2022 Temporal processing",
      "vv": {
        "nodeType": "component",
        "shape": "rounded",
        "component": "VideoProcessor",
        "fill": "#06b6d4",
        "stroke": "#06b6d4"
      }
    },
    {
      "id": "config",
      "type": "text",
      "x": -200,
      "y": 600,
      "width": 250,
      "height": 120,
      "color": "3",
      "text": "Qwen2VLConfig\nconfiguration_qwen2_vl.py:239\n\u2022 vision_config\n\u2022 text_config\n\u2022 token IDs",
      "vv": {
        "nodeType": "component",
        "shape": "rounded",
        "component": "Config",
        "fill": "#fbbf24",
        "stroke": "#fbbf24"
      }
    },
    {
      "id": "for_conditional_gen",
      "type": "text",
      "x": 450,
      "y": 500,
      "width": 400,
      "height": 120,
      "color": "4",
      "text": "Qwen2VLForConditionalGeneration\nmodeling_qwen2_vl.py:1263\nInherits: Qwen2VLPreTrainedModel, GenerationMixin\n\u2022 forward() - main entry point\n\u2022 lm_head - output projection",
      "vv": {
        "nodeType": "component",
        "shape": "rounded",
        "component": "Model",
        "fill": "#10b981",
        "stroke": "#10b981"
      }
    },
    {
      "id": "qwen2vl_model",
      "type": "text",
      "x": 450,
      "y": 680,
      "width": 400,
      "height": 120,
      "color": "4",
      "text": "Qwen2VLModel\nmodeling_qwen2_vl.py:925\n\u2022 self.visual (vision encoder)\n\u2022 self.language_model (text decoder)\n\u2022 get_rope_index() - 3D position encoding",
      "vv": {
        "nodeType": "component",
        "shape": "rounded",
        "component": "Model",
        "fill": "#10b981",
        "stroke": "#10b981"
      }
    },
    {
      "id": "vision_transformer",
      "type": "text",
      "x": 100,
      "y": 880,
      "width": 300,
      "height": 180,
      "color": "4",
      "text": "Qwen2VisionTransformer\nmodeling_qwen2_vl.py:669\nInherits: Qwen2VLPreTrainedModel\n\n\u2022 PatchEmbed:257 - patch tokenization\n\u2022 VisionRotaryEmbedding:243\n\u2022 Qwen2VLVisionBlock:432 (\u00d732)\n  - VisionAttention:349\n  - VisionMlp:299\n\u2022 PatchMerger:283 - spatial merge",
      "vv": {
        "nodeType": "component",
        "shape": "rounded",
        "component": "VisionEncoder",
        "fill": "#10b981",
        "stroke": "#10b981"
      }
    },
    {
      "id": "text_model",
      "type": "text",
      "x": 450,
      "y": 880,
      "width": 300,
      "height": 180,
      "color": "4",
      "text": "Qwen2VLTextModel\nmodeling_qwen2_vl.py:768\nInherits: Qwen2VLPreTrainedModel\n\n\u2022 embed_tokens - embeddings\n\u2022 Qwen2VLDecoderLayer:570 (\u00d780)\n  - Qwen2VLAttention:478 (GQA)\n  - Qwen2MLP:462 (SwiGLU)\n\u2022 Qwen2RMSNorm - normalization",
      "vv": {
        "nodeType": "component",
        "shape": "rounded",
        "component": "LanguageModel",
        "fill": "#10b981",
        "stroke": "#10b981"
      }
    },
    {
      "id": "multimodal_fusion",
      "type": "text",
      "x": 800,
      "y": 880,
      "width": 280,
      "height": 180,
      "color": "6",
      "text": "Multimodal Fusion\nIn Qwen2VLModel\n\n\u2022 Vision embeddings merged\n\u2022 Text embeddings\n\u2022 Combined via position IDs\n\u2022 3D RoPE for vision\n\u2022 1D RoPE for text",
      "vv": {
        "nodeType": "component",
        "shape": "rounded",
        "component": "MultimodalProjector",
        "fill": "#a855f7",
        "stroke": "#a855f7"
      }
    },
    {
      "id": "generation_mixin",
      "type": "text",
      "x": 900,
      "y": 500,
      "width": 260,
      "height": 120,
      "color": "6",
      "text": "GenerationMixin\n(inherited)\n\u2022 generate()\n\u2022 beam search\n\u2022 sampling strategies",
      "vv": {
        "nodeType": "component",
        "shape": "rounded",
        "component": "GenerationMixin",
        "fill": "#a855f7",
        "stroke": "#a855f7"
      }
    },
    {
      "id": "model_output",
      "type": "text",
      "x": 450,
      "y": 1140,
      "width": 400,
      "height": 100,
      "color": "2",
      "text": "Qwen2VLCausalLMOutputWithPast\nmodeling_qwen2_vl.py:86\n\u2022 logits, loss\n\u2022 past_key_values (KV cache)\n\u2022 rope_deltas",
      "vv": {
        "nodeType": "component",
        "shape": "rounded",
        "component": "ModelOutput",
        "fill": "#ff9800",
        "stroke": "#ff9800"
      }
    },
    {
      "id": "pretrained_model",
      "type": "text",
      "x": 100,
      "y": 600,
      "width": 250,
      "height": 100,
      "color": "1",
      "text": "PreTrainedModel\n(transformers base)\n\u2022 from_pretrained()\n\u2022 save_pretrained()",
      "vv": {
        "nodeType": "component",
        "shape": "rounded",
        "component": "PreTrainedModel",
        "fill": "#ef4444",
        "stroke": "#ef4444"
      }
    },
    {
      "id": "image_utils",
      "type": "text",
      "x": -200,
      "y": 320,
      "width": 220,
      "height": 100,
      "color": "5",
      "text": "image_utils.py\nimage_transforms.py\n\u2022 resize, rescale\n\u2022 normalize, pad\n\u2022 convert_to_rgb",
      "vv": {
        "nodeType": "component",
        "shape": "rounded",
        "component": "ImageTransform",
        "fill": "#06b6d4",
        "stroke": "#06b6d4"
      }
    }
  ],
  "edges": [
    {
      "id": "e1",
      "fromNode": "user_input",
      "toNode": "processor",
      "color": "5",
      "vv": {
        "stroke": "#06b6d4",
        "edgeType": "processes"
      }
    },
    {
      "id": "e2",
      "fromNode": "processor",
      "toNode": "image_processor",
      "color": "4",
      "vv": {
        "stroke": "#10b981",
        "edgeType": "uses"
      }
    },
    {
      "id": "e3",
      "fromNode": "processor",
      "toNode": "video_processor",
      "color": "4",
      "vv": {
        "stroke": "#10b981",
        "edgeType": "uses"
      }
    },
    {
      "id": "e4",
      "fromNode": "processor",
      "toNode": "tokenizer",
      "color": "4",
      "vv": {
        "stroke": "#10b981",
        "edgeType": "uses"
      }
    },
    {
      "id": "e5",
      "fromNode": "image_processor",
      "toNode": "image_utils",
      "color": "4",
      "vv": {
        "stroke": "#10b981",
        "edgeType": "uses"
      }
    },
    {
      "id": "e6",
      "fromNode": "processor",
      "toNode": "for_conditional_gen",
      "color": "5",
      "vv": {
        "stroke": "#06b6d4",
        "edgeType": "processes"
      }
    },
    {
      "id": "e7",
      "fromNode": "for_conditional_gen",
      "toNode": "qwen2vl_model",
      "color": "4",
      "vv": {
        "stroke": "#10b981",
        "edgeType": "uses"
      }
    },
    {
      "id": "e8",
      "fromNode": "qwen2vl_model",
      "toNode": "vision_transformer",
      "color": "4",
      "vv": {
        "stroke": "#10b981",
        "edgeType": "uses"
      }
    },
    {
      "id": "e9",
      "fromNode": "qwen2vl_model",
      "toNode": "text_model",
      "color": "4",
      "vv": {
        "stroke": "#10b981",
        "edgeType": "uses"
      }
    },
    {
      "id": "e10",
      "fromNode": "vision_transformer",
      "toNode": "multimodal_fusion",
      "color": "5",
      "vv": {
        "stroke": "#06b6d4",
        "edgeType": "encodes"
      }
    },
    {
      "id": "e11",
      "fromNode": "text_model",
      "toNode": "multimodal_fusion",
      "color": "5",
      "vv": {
        "stroke": "#06b6d4",
        "edgeType": "encodes"
      }
    },
    {
      "id": "e12",
      "fromNode": "multimodal_fusion",
      "toNode": "model_output",
      "color": "5",
      "vv": {
        "stroke": "#06b6d4",
        "edgeType": "generates"
      }
    },
    {
      "id": "e13",
      "fromNode": "for_conditional_gen",
      "toNode": "generation_mixin",
      "color": "1",
      "vv": {
        "stroke": "#ef4444",
        "edgeType": "extends"
      }
    },
    {
      "id": "e14",
      "fromNode": "for_conditional_gen",
      "toNode": "pretrained_model",
      "color": "1",
      "vv": {
        "stroke": "#ef4444",
        "edgeType": "extends"
      }
    },
    {
      "id": "e15",
      "fromNode": "config",
      "toNode": "for_conditional_gen",
      "color": "3",
      "vv": {
        "stroke": "#fbbf24",
        "edgeType": "configures"
      }
    },
    {
      "id": "e16",
      "fromNode": "qwen2vl_model",
      "toNode": "model_output",
      "color": "5",
      "vv": {
        "stroke": "#06b6d4",
        "edgeType": "generates"
      }
    }
  ]
}